{
    "dataset": "amazon",
    "model": {
        "name": "lstm",
        "embeddings": "glove300",
        "hidden_size": 200,
        "dropout": 0.0
    },
    "tokenizer": {
        "name": "word_tokenizer",
        "max_sent_len": 512,
        "max_tokens": 60000,
        "freq_cutoff": 1,
        "pad_token": "[PAD]",
        "unk_token": "[UNK]"
    },
    "train_kwargs": {
        "max_epochs": 500,
        "train_batch_size": 32,
        "dev_batch_size": 64,
        "learning_rate": 0.0001,
        "patience": 10,
        "train_eval_every": 50,
        "dev_eval_every": 500,
        "device": "cuda:1"
    }
}