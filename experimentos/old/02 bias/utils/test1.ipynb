{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61762004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from vectorizers import *\n",
    "# from datasets import InterTASS2019task1Dataset as tass, Melisa2Dataset\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eb476d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ab1ef578bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tass' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = tass().get_train_dataframe(lang=['es']).iloc[:5]\n",
    "df_test = tass().get_train_dataframe(lang=['es']).iloc[11:15].reset_index(drop=True)\n",
    "df_train['label'] = df_train['label'].replace({'P': 1., 'N': 0.}) \n",
    "df_test['label'] = df_test['label'].replace({'P': 1., 'N': 0.}) \n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be90b922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 14027.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"cómetelo\"': 19,\n",
      " '3.': 26,\n",
      " '@AlbaBenito99': 8,\n",
      " '@Yulian_Poe': 60,\n",
      " '@guillermoterry1': 59,\n",
      " '@myendlesshazza': 68,\n",
      " '@toNi_end': 49,\n",
      " 'Ah.': 58,\n",
      " 'JAJAJAJA': 13,\n",
      " 'Me': 37,\n",
      " 'Quiero': 18,\n",
      " 'Vale': 17,\n",
      " 'a': 3,\n",
      " 'a.': 33,\n",
      " 'ahí': 20,\n",
      " 'b.': 30,\n",
      " 'basura': 42,\n",
      " 'bebiendose': 67,\n",
      " 'bien': 36,\n",
      " 'contesta': 14,\n",
      " 'dado': 63,\n",
      " 'de': 45,\n",
      " 'dejasen': 46,\n",
      " 'el': 22,\n",
      " 'emitir': 44,\n",
      " 'entendido': 50,\n",
      " 'esa': 43,\n",
      " 'escribo': 31,\n",
      " 'eso': 11,\n",
      " 'evolucionar': 39,\n",
      " 'grima': 61,\n",
      " 'ha': 25,\n",
      " 'habías': 51,\n",
      " 'hay': 40,\n",
      " 'he': 34,\n",
      " 'help': 27,\n",
      " 'hs': 64,\n",
      " 'incluyo.': 52,\n",
      " 'la': 35,\n",
      " 'lo': 2,\n",
      " 'los': 15,\n",
      " 'mal': 4,\n",
      " 'me': 5,\n",
      " 'mejor': 47,\n",
      " 'mogollón': 7,\n",
      " 'mucho': 57,\n",
      " 'muchs': 62,\n",
      " 'más': 56,\n",
      " 'para': 38,\n",
      " 'pero': 9,\n",
      " 'por': 6,\n",
      " 'puto': 32,\n",
      " 'que': 0,\n",
      " 'quedado': 24,\n",
      " 'raro': 23,\n",
      " 'regla': 65,\n",
      " 'rápido': 12,\n",
      " 'seria': 48,\n",
      " 'sigo': 29,\n",
      " 'sobretodo': 10,\n",
      " 'solo': 54,\n",
      " 'su': 66,\n",
      " 'supuesto!': 55,\n",
      " 'surrando': 28,\n",
      " 'tia': 53,\n",
      " 'visto': 21,\n",
      " 'wasaps': 16,\n",
      " 'y': 1,\n",
      " 'ya': 41}\n",
      "[[1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 1 2 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0]\n",
      " [1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [2 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vec = BONgramsVectorizer(tokenizer=lambda x: x.split(), \n",
    "                         min_count=1., max_count=None, \n",
    "                         max_words=None, ngram_range=(1,1),\n",
    "                         vocab=None, \n",
    "                         unk_token=None)\n",
    "X_train = vec.fit_transform(df_train['text'])\n",
    "X_train = X_train.toarray()\n",
    "pprint(vec.vocab)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3e995df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 8837.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test1():\n",
    "    vec = BONgramsVectorizer(tokenizer=lambda x: x.split(), \n",
    "                         min_count=1., max_count=None, \n",
    "                         max_words=None, ngram_range=(1,1),\n",
    "                         vocab=None, \n",
    "                         unk_token=None)\n",
    "    X_train = vec.fit_transform(df_train['text']).toarray()\n",
    "    inv_vocab = {idx:tk for tk,idx in vec.vocab.items()}\n",
    "    for j in range(len(df_train['text'])):\n",
    "        string_tk = df_train['text'][j].split()\n",
    "        for i, n in enumerate(X_train[j,:]):\n",
    "            if n != string_tk.count(inv_vocab[i]):\n",
    "                success = False\n",
    "                break\n",
    "            else:\n",
    "                success = True\n",
    "    if success:\n",
    "        print('passed!')\n",
    "    else:\n",
    "        print('not passed')\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbefbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 8263.01it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13906.84it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 14074.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 2 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0]\n",
      " [0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test2():\n",
    "    \n",
    "    vec1 = BONgramsVectorizer(tokenizer=lambda x: x.split(), \n",
    "                             min_count=0., max_count=None, \n",
    "                             max_words=None, ngram_range=(1,1),\n",
    "                             vocab=None, \n",
    "                             unk_token=None)\n",
    "    X_train1 = vec1.fit_transform(df_train['text'])\n",
    "    X_train1 = X_train1.toarray()\n",
    "\n",
    "    max_freq = df_train['text'].str.split().explode().value_counts()[0]\n",
    "    \n",
    "    vec2 = BONgramsVectorizer(tokenizer=lambda x: x.split(), \n",
    "                             min_count=1., max_count=max_freq, \n",
    "                             max_words=None, ngram_range=(1,1),\n",
    "                             vocab=None, \n",
    "                             unk_token=None)\n",
    "    X_train2 = vec2.fit_transform(df_train['text'])\n",
    "    X_train2 = X_train2.toarray()\n",
    "    \n",
    "    vec3 = BONgramsVectorizer(tokenizer=lambda x: x.split(), \n",
    "                             min_count=1., max_count=max_freq-1, \n",
    "                             max_words=None, ngram_range=(1,1),\n",
    "                             vocab=None, \n",
    "                             unk_token=None)\n",
    "    X_train3 = vec3.fit_transform(df_train['text'])\n",
    "    X_train3 = X_train3.toarray()\n",
    "    \n",
    "    if (np.array_equal(X_train1,X_train2)) and not (np.array_equal(X_train1,X_train3)):\n",
    "        print('passed!')\n",
    "    else:\n",
    "        print('not passed')\n",
    "\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d74f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>review_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excelente producto pues el material es metálic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>los almohadones son muy lindos y se adecúan mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lo que buscaba en precio y calidad.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funciona como esta descrito, buena sincronizac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conozco el q cuesta un 60% mas y la unica dife...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no era lo que esperaba, no sirven para metal, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>el producto cumple con las expectativas. ya se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>muy bonitas y bien construidas. contento con l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>estoy muy contento. tuve que hacer unos ajuste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>producto malo , la chapa parece de lata de pic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content  review_rate\n",
       "0  excelente producto pues el material es metálic...            1\n",
       "1  los almohadones son muy lindos y se adecúan mu...            1\n",
       "2                lo que buscaba en precio y calidad.            0\n",
       "3  funciona como esta descrito, buena sincronizac...            1\n",
       "4  conozco el q cuesta un 60% mas y la unica dife...            1\n",
       "5  no era lo que esperaba, no sirven para metal, ...            0\n",
       "6  el producto cumple con las expectativas. ya se...            1\n",
       "7  muy bonitas y bien construidas. contento con l...            1\n",
       "8  estoy muy contento. tuve que hacer unos ajuste...            1\n",
       "9  producto malo , la chapa parece de lata de pic...            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate']).sample(n=10,random_state=0).reset_index(drop=True)\n",
    "df_train['review_content'] = df_train['review_content'].str.lower()\n",
    "df_val = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate']).sample(n=10,random_state=72384).reset_index(drop=True)\n",
    "df_val['review_content'] = df_val['review_content'].str.lower()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "763f9e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1115.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 1 1 0 2 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [4 4 2 4 3 5 1 0 0 2 3 0 0 1 2 2 1 2 2 0 0 2 1 1 0 0 2 1 0 0 0 1 0 0 0 0\n",
      "  1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [3 1 2 0 2 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [3 0 1 2 2 1 1 1 2 2 0 2 0 2 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1]\n",
      " [1 2 1 0 0 0 0 1 2 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [4 0 3 1 2 2 2 5 2 1 1 1 2 2 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
      "  0 1 0 0 2 0 0 0 0 0 0 1 0 0 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [4 4 4 1 1 1 1 1 1 0 1 1 0 0 2 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [5 1 1 2 1 2 3 2 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 1 1 0 0 0 0 1 0 0 0 1 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 4 2 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " ',': 1,\n",
       " 'de': 2,\n",
       " 'y': 3,\n",
       " 'la': 4,\n",
       " 'muy': 5,\n",
       " 'el': 6,\n",
       " 'que': 7,\n",
       " 'no': 8,\n",
       " 'es': 9,\n",
       " 'las': 10,\n",
       " 'en': 11,\n",
       " 'lo': 12,\n",
       " 'se': 13,\n",
       " 'bien': 14,\n",
       " 'calidad': 15,\n",
       " 'con': 16,\n",
       " 'los': 17,\n",
       " 'muy bien': 18,\n",
       " 'para': 19,\n",
       " '. en': 20,\n",
       " 'son': 21,\n",
       " 'con la': 22,\n",
       " 'a': 23,\n",
       " 'tener': 24,\n",
       " 'producto': 25,\n",
       " ', y': 26,\n",
       " ', las': 27,\n",
       " 'precio': 28,\n",
       " 'un': 29,\n",
       " 'no lo': 30,\n",
       " 'por': 31,\n",
       " 'resistente': 32,\n",
       " 'lo que': 33,\n",
       " 'contento': 34,\n",
       " 'era': 35,\n",
       " 'buena': 36,\n",
       " 'mi': 37,\n",
       " 'en relacion': 38,\n",
       " 'material': 39,\n",
       " 'ya': 40,\n",
       " 'compra': 41,\n",
       " 'compra .': 42,\n",
       " 'la compra': 43,\n",
       " 'el material': 44,\n",
       " 'malo': 45,\n",
       " 'varios': 46,\n",
       " 'si': 47,\n",
       " '. el': 48,\n",
       " 'potencia': 49,\n",
       " 'estoy': 50,\n",
       " 'o': 51,\n",
       " 'me': 52,\n",
       " 'solo': 53,\n",
       " 'tengo': 54,\n",
       " ', no': 55,\n",
       " 'relacion': 56,\n",
       " 'tienen': 57,\n",
       " 'el q': 58,\n",
       " 'minutos': 59,\n",
       " '10': 60,\n",
       " 'arma': 61,\n",
       " 'madera': 62,\n",
       " 'firme': 63,\n",
       " 'conozco el': 64,\n",
       " 'diferencia': 65,\n",
       " 've': 66,\n",
       " 'q cuesta': 67,\n",
       " 'super': 68,\n",
       " 'plastico': 69,\n",
       " 'tan': 70,\n",
       " 'tiene': 71,\n",
       " 'bordes': 72,\n",
       " 'menos': 73,\n",
       " 'dinero .': 74,\n",
       " 'unica': 75,\n",
       " '. la': 76,\n",
       " 'varios modos': 77,\n",
       " 'modos de': 78,\n",
       " 'de transición': 79,\n",
       " 'transición muy': 80,\n",
       " 'muy llamativo': 81,\n",
       " 'llamativo pero': 82,\n",
       " 'pero falto': 83,\n",
       " 'falto de': 84,\n",
       " 'de potencia': 85,\n",
       " 'potencia .': 86,\n",
       " 'la claridad': 87,\n",
       " 'mas': 88,\n",
       " 'claridad es': 89,\n",
       " 'es la': 90,\n",
       " 'la esperada': 91,\n",
       " 'esperada .': 92,\n",
       " 'conozco': 93,\n",
       " 'q': 94,\n",
       " 'un 60': 95,\n",
       " 'cuesta': 96,\n",
       " '60': 97,\n",
       " '%': 98,\n",
       " 'cuesta un': 99}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "vec = BONgramsVectorizer(tokenizer=word_tokenize, \n",
    "                          min_count=0., max_count=None, \n",
    "                          max_words=100, ngram_range=(1,2),\n",
    "                          vocab=None, unk_token=None)\n",
    "\n",
    "X_train = vec.fit_transform(df_train['review_content']).toarray()\n",
    "print(X_train)\n",
    "vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed8308db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1195.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 667.56it/s]\n",
      "<ipython-input-29-f74ba17fe69c>:16: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  X_train2[1:,:] == X_train1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['.',',','de','y','la','muy','el','que','no','es','las','en','lo','se']\n",
    "\n",
    "vec1 = BONgramsVectorizer(tokenizer=word_tokenize, \n",
    "                          min_count=0., max_count=None, \n",
    "                          max_words=100, ngram_range=(1,3),\n",
    "                          vocab=vocab, unk_token=None)\n",
    "\n",
    "X_train1 = vec1.fit_transform(df_train['review_content']).toarray()\n",
    "\n",
    "vec2 = BONgramsVectorizer(tokenizer=word_tokenize, \n",
    "                          min_count=0., max_count=None, \n",
    "                          max_words=100, ngram_range=(1,3),\n",
    "                          vocab=vocab, unk_token='UNK')\n",
    "\n",
    "X_train2 = vec2.fit_transform(df_train['review_content']).toarray()\n",
    "X_train2[:,1:] == X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53570cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1481.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excelente 1\n",
      "producto 1\n",
      "pues 1\n",
      "el 1\n",
      "material 1\n",
      "es 1\n",
      "metálico 1\n",
      "y 1\n",
      "resistente 1\n",
      ", 1\n",
      "durabilidad 1\n",
      "comprobada 1\n",
      "calza 1\n",
      "a 1\n",
      "la 1\n",
      "perfección 1\n",
      "en 1\n",
      "optra 1\n",
      "designe 1\n",
      ". 1\n",
      "los 1\n",
      "almohadones 1\n",
      "son 1\n",
      "muy 1\n",
      "lindos 1\n",
      "se 1\n",
      "adecúan 1\n",
      "bien 1\n",
      "las 1\n",
      "sillas 1\n",
      "fundas 1\n",
      "excelentes 1\n",
      "por 1\n",
      "calidad 1\n",
      "de 1\n",
      "tela 1\n",
      "costuras 1\n",
      "fuertes 1\n",
      "cierres 1\n",
      "terminados 1\n",
      "relleno 1\n",
      "buena 1\n",
      "cómodo 1\n",
      "mullido 1\n",
      "quedamos 1\n",
      "conformes 1\n",
      "con 1\n",
      "compra 1\n",
      "lo 1\n",
      "que 1\n",
      "buscaba 1\n",
      "precio 1\n",
      "funciona 1\n",
      "como 1\n",
      "esta 1\n",
      "descrito 1\n",
      "sincronizacion 1\n",
      "bluetooth 1\n",
      "varios 1\n",
      "modos 1\n",
      "transición 1\n",
      "llamativo 1\n",
      "pero 1\n",
      "falto 1\n",
      "potencia 1\n",
      "claridad 1\n",
      "esperada 1\n",
      "conozco 1\n",
      "q 1\n",
      "cuesta 1\n",
      "un 1\n",
      "60 1\n",
      "% 1\n",
      "mas 1\n",
      "unica 1\n",
      "diferencia 1\n",
      "madera 1\n",
      "no 1\n",
      "ve 1\n",
      "tan 1\n",
      "tiene 1\n",
      "bordes 1\n",
      "plastico 1\n",
      "super 1\n",
      "firme 1\n",
      "relacion 1\n",
      "arma 1\n",
      "10 1\n",
      "minutos 1\n",
      "o 1\n",
      "menos 1\n",
      "era 1\n",
      "esperaba 1\n",
      "sirven 1\n",
      "para 1\n",
      "metal 1\n",
      "solo 1\n",
      "tengo 1\n",
      "adorno 1\n",
      "cumple 1\n",
      "expectativas 1\n",
      "ya 1\n",
      "sabe 1\n",
      "está 1\n",
      "comprando 1\n",
      "algo 1\n",
      "económico 1\n",
      "mi 1\n",
      "caso 1\n",
      "idea 1\n",
      "tener 1\n",
      "reproductor 1\n",
      "liviano 1\n",
      "todo 1\n",
      "terreno 1\n",
      "me 1\n",
      "importe 1\n",
      "si 1\n",
      "mala 1\n",
      "suerte 1\n",
      "perderlo 1\n",
      "roben 1\n",
      "importante 1\n",
      "comprar 1\n",
      "aparte 1\n",
      "auriculares 1\n",
      "mejor 1\n",
      "trae 1\n",
      "equipo 1\n",
      "bonitas 1\n",
      "construidas 1\n",
      "contento 1\n",
      "consejo 1\n",
      "quienes 1\n",
      "compren 1\n",
      ": 1\n",
      "vean 1\n",
      "espacio 1\n",
      "quieren 1\n",
      "iluminar 1\n",
      "estas 1\n",
      "ampolletas 1\n",
      "tienen 1\n",
      "gran 1\n",
      "lumínica 1\n",
      "general 1\n",
      "estos 1\n",
      "tipos 1\n",
      "'vintage 1\n",
      "' 1\n",
      "nunca 1\n",
      "van 1\n",
      "este 1\n",
      "tipo 1\n",
      "ampolleta 1\n",
      "consideren 1\n",
      "una 1\n",
      "lámpara 1\n",
      "soquetes 1\n",
      "( 1\n",
      "3 1\n",
      "hacia 1\n",
      "arriba 1\n",
      ") 1\n",
      "estoy 1\n",
      "tuve 1\n",
      "hacer 1\n",
      "unos 1\n",
      "ajustes 1\n",
      "respondio 1\n",
      "noble 1\n",
      "usando 1\n",
      "disfrutsndo 1\n",
      "cortar 1\n",
      "cesped 1\n",
      "luego 1\n",
      "recojerlo 1\n",
      "junta 1\n",
      "pasto 1\n",
      "bueno 1\n",
      "tambie 1\n",
      "carcaza 1\n",
      "malo 1\n",
      "chapa 1\n",
      "parece 1\n",
      "lata 1\n",
      "picadillo 1\n",
      "soldaduras 1\n",
      "defectuosas 1\n",
      "plegado 1\n",
      "recomiendo 1\n",
      "nada 1\n",
      "malgaste 1\n",
      "dinero 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'el': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = BONgramsVectorizer(tokenizer=word_tokenize, \n",
    "                          min_count=0., max_count=None, \n",
    "                          max_words=20, ngram_range=(1,1),\n",
    "                          vocab=['coso','pituto','el','cosito'], unk_token=None)\n",
    "\n",
    "X_train = vec.fit_transform(df_train['review_content']).toarray()\n",
    "X_train\n",
    "vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec6ee932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1729.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[57,  3],\n",
       "       [92,  2],\n",
       "       [41,  2],\n",
       "       [18,  0],\n",
       "       [15,  0],\n",
       "       [47,  1],\n",
       "       [15,  1],\n",
       "       [12,  0],\n",
       "       [53,  0],\n",
       "       [15,  0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = vec.transform(df_val['review_content']).toarray()\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "262d65c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 2, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec2 = CountVectorizer(tokenizer=word_tokenize,\n",
    "                       max_features=20,ngram_range=(1,2),\n",
    "                       vocabulary=['coso','pituto','el','cosito'])\n",
    "\n",
    "X_train = vec2.fit_transform(df_train['review_content']).toarray()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d89d16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 8.]\n",
      " [7. 6.]\n",
      " [7. 7.]\n",
      " [5. 7.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14., 14.],\n",
       "       [12., 15.],\n",
       "       [21., 18.],\n",
       "       [ 0.,  0.],\n",
       "       [17., 22.],\n",
       "       [ 0.,  0.],\n",
       "       [14., 14.],\n",
       "       [56., 72.],\n",
       "       [21., 21.],\n",
       "       [ 7.,  7.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array([1,0,0,1,0,1,1,1,0,0],dtype=float)\n",
    "y_train = y_train.astype(int)\n",
    "y_one_hot = np.zeros((y_train.size,y_train.max()+1),dtype=float)\n",
    "y_one_hot[np.arange(y_train.size),y_train] = 1.\n",
    "\n",
    "X = np.array([[0, 0, 2, 0],\n",
    "               [1, 0, 1, 0],\n",
    "               [0, 3, 0, 0],\n",
    "               [0, 0, 0, 0],\n",
    "               [1, 0, 1, 1],\n",
    "               [0, 0, 0, 0],\n",
    "               [0, 0, 2, 0],\n",
    "               [4, 2, 1, 3],\n",
    "               [0, 0, 3, 0],\n",
    "               [0, 0, 1, 0]])\n",
    "from scipy.sparse import csr_matrix\n",
    "X = csr_matrix(X)\n",
    "\n",
    "X.dot(X.minimum(1).T.dot(y_one_hot))\n",
    "print(W)\n",
    "X.dot(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf515a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate'])#.sample(n=10000,random_state=0).reset_index(drop=True)\n",
    "df_train['review_content'] = df_train['review_content'].str.lower()\n",
    "df_val = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate'])#.sample(n=10000,random_state=72384).reset_index(drop=True)\n",
    "df_val['review_content'] = df_val['review_content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c68c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 767573.,  888766.],\n",
       "       [1138319., 1909307.],\n",
       "       [1580805., 2728291.],\n",
       "       ...,\n",
       "       [ 334374.,  473914.],\n",
       "       [1924806., 3430821.],\n",
       "       [1114427., 1977380.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = WordByCategoryCBOWVectorizer()\n",
    "X_train = vec.fit_transform(df_train['review_content'],df_train['review_rate'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c44f4662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([3, 3, 0, 0, 5, 1, 1, 4, 4, 2, 2]), batch_sizes=tensor([2, 2, 2, 2, 2, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "import torch\n",
    "\n",
    "vec = CountVectorizer(tokenizer=word_tokenize)\n",
    "corpus = ['Mi casa es muy linda', 'Mi casa también es muy linda']\n",
    "X = vec.fit_transform(corpus)\n",
    "tokenize = vec.build_analyzer()\n",
    "vocab = defaultdict(lambda : -1,vec.vocabulary_)\n",
    "idx_seqs = [torch.tensor([vocab[tk] for tk in tokenize(text)]) for text in corpus]\n",
    "pack_sequence(idx_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9f7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds = pd.Series(['Mi casa es muy linda', 'Mi casa también es muy linda'])\n",
    "ds.str.len().argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fd86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>review_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excelente producto pues el material es metálic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>los almohadones son muy lindos y se adecúan mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lo que buscaba en precio y calidad.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funciona como esta descrito, buena sincronizac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conozco el q cuesta un 60% mas y la unica dife...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no era lo que esperaba, no sirven para metal, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>el producto cumple con las expectativas. ya se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>muy bonitas y bien construidas. contento con l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>estoy muy contento. tuve que hacer unos ajuste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>producto malo , la chapa parece de lata de pic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content  review_rate\n",
       "0  excelente producto pues el material es metálic...            1\n",
       "1  los almohadones son muy lindos y se adecúan mu...            1\n",
       "2                lo que buscaba en precio y calidad.            0\n",
       "3  funciona como esta descrito, buena sincronizac...            1\n",
       "4  conozco el q cuesta un 60% mas y la unica dife...            1\n",
       "5  no era lo que esperaba, no sirven para metal, ...            0\n",
       "6  el producto cumple con las expectativas. ya se...            1\n",
       "7  muy bonitas y bien construidas. contento con l...            1\n",
       "8  estoy muy contento. tuve que hacer unos ajuste...            1\n",
       "9  producto malo , la chapa parece de lata de pic...            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate']).sample(n=10,random_state=0).reset_index(drop=True)\n",
    "df_train['review_content'] = df_train['review_content'].str.lower()\n",
    "df_val = Melisa2Dataset().get_train_dataframe(usecols=['review_content','review_rate']).sample(n=10,random_state=72384).reset_index(drop=True)\n",
    "df_val['review_content'] = df_val['review_content'].str.lower()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb64faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 65, 141,  54,  44,  97,  81,   6, 193, 157, 156, 144, 157,  78,  40,\n",
       "          15, 122,  64,   6,  66, 118,  33,  96,  91,  68, 171, 183, 152, 122,\n",
       "         100, 179, 174, 192, 144, 124, 113,  94, 158, 172,  96, 108, 166,  56,\n",
       "         132, 127,  56, 144, 113, 102, 155,   6,  69,  93,  41,  19,  22,  56,\n",
       "         114,  30, 193, 144, 103, 144, 180,  65,  67, 124, 102, 164,   6],\n",
       "        [122,  25, 192,  23,  49,   6,  50,  44,  96,  39,   6,  47, 129, 146,\n",
       "          97,  42,  10, 191,  23,  65,  70, 144, 147,  92,   5,  74,  18,   5,\n",
       "         124, 176,  88, 139, 105,   5,  66,  87,  76, 178,   2,   1, 126, 176,\n",
       "           6, 158, 163, 188,  11, 171,  56,  75, 177,  56,  17,   5,  48, 184,\n",
       "         106,  56, 189, 165,   3,  56,   8,  90,  21,   4,   6,   0,   0],\n",
       "        [103,  16, 164, 122,  99, 192, 157,  12, 122,  23,  11,  97, 159,   6,\n",
       "          97,  86, 164,  80, 138,  96,  30,  56,  96, 170,   5,  97,  52,  84,\n",
       "           5, 103,  36,   5, 192, 122,  23, 173,   6,  65, 151,  69,  56, 122,\n",
       "          27,  30,   5, 192,  69,  55, 192, 121,   6, 145, 122,  45,  44,  96,\n",
       "          39,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 46,  65, 143,  53, 183,   9,   0, 111, 192,  96, 185,  60,  69, 144,\n",
       "          96, 107, 124, 157, 190, 169, 153, 192, 124, 175, 103,  26,  56, 136,\n",
       "          69, 167,  83,   6,   6,  66, 150,  30, 140, 122,  23, 157,  20,  66,\n",
       "           7, 119, 127, 115,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 77, 122,  50,   6, 182, 144,  89, 186,  14,   6,  65, 112, 154, 125,\n",
       "           6, 102,  77, 187, 192,  62,  56,  51,  65,  34, 192, 104, 124, 171,\n",
       "         144, 148,   5,  65,  95, 131, 122,  28, 168,   6,  66, 150,  44,  96,\n",
       "          32,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [141, 110,   5,  96,  35, 130,  56,  98,  56, 135,   5,  97, 162,  57,\n",
       "           5, 192,  65, 137,  69, 110, 124, 102, 149, 129, 123,   5, 109, 118,\n",
       "          61,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 85,  38,  73,  58,   5,  27, 160, 138,  24,   6, 189, 120,  56, 181,\n",
       "         122, 101, 134,  82,  56, 139,   6,  96,  37,  69,  96,  72,   6,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 79, 141, 142,  65, 112,  69, 117, 192, 153,   5,  63,  43,   5,  31,\n",
       "          11,  96, 133,  66,  65, 128,  59,   6,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [124,  68, 102, 144,  71,   5, 124, 161, 129, 116,   5, 163,  97, 172,\n",
       "          56,  13,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [102, 144,  29,  66, 140, 192,  30,   6,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Word2IndexVectorizer(object):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.vec = CountVectorizer(*args,**kwargs)\n",
    "        self.tokenize = self.vec.build_analyzer()\n",
    "\n",
    "    def fit_transform(self,ds_train):\n",
    "        self.vec.fit(ds_train)\n",
    "        vocab = defaultdict(lambda : -1,self.vec.vocabulary_)\n",
    "        self.vocab = vocab\n",
    "        ds_train = ds_train.apply(self.tokenize)\n",
    "        sorted_idx = ds_train.str.len().argsort()\n",
    "        idx_seqs = np.array([torch.tensor([vocab[tk] for tk in text]) for text in ds_train],dtype=object)\n",
    "        idx_seqs = idx_seqs[sorted_idx][::-1]\n",
    "        seqs = pack_sequence(idx_seqs)\n",
    "        return seqs\n",
    "\n",
    "    def transform(self,ds_test):\n",
    "        ds_train = ds_train.apply(self.tokenize)\n",
    "        sorted_idx = ds_train.str.len().argsort()\n",
    "        idx_seqs = np.array([torch.tensor([vocab[tk] for tk in text]) for text in ds_train],dtype=object)\n",
    "        idx_seqs = idx_seqs[sorted_idx][::-1]\n",
    "        seqs = pack_sequence(idx_seqs)\n",
    "        return seqs\n",
    "        \n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "        \n",
    "vec = Word2IndexVectorizer(tokenizer=word_tokenize)\n",
    "packed_seq = vec.fit_transform(df_train['review_content'])\n",
    "padded_seq, lenghts = pad_packed_sequence(packed_seq,batch_first=True)\n",
    "\n",
    "padded_seq.to(device=torch.device('cpu'))\n",
    "# emb = nn.Embedding(len(vec.vocab),2)\n",
    "# emb(padded_seq).size()\n",
    "#padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a8540c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b842961f85a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(4,2,padding_idx=-1)\n",
    "emb(torch.tensor([[0,0,0,3],[1,1,1,-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
